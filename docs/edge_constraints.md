ğŸš€ Edge Constraints & Deployment Considerations

This project is designed with edge deployment constraints in mind, where compute, memory, and power are limited.
All Phase-1 design decisions prioritize feasibility, portability, and reliability over aggressive optimization.

ğŸ“¦ Model Size & Efficiency

A MobileNetV2 architecture is selected due to its lightweight design and strong suitability for edge inference.

The Phase-1 model uses a frozen backbone with a compact classification head, resulting in a small model footprint suitable for resource-constrained devices.

Why this matters:

ğŸ§  Low parameter count

ğŸ’¾ Reduced memory footprint

âš¡ Stable CPU-only inference

ğŸ§® Compute & Memory Constraints

Edge devices typically operate with limited CPU capability and restricted RAM.

MobileNetV2 employs depthwise separable convolutions, which significantly reduce:

ğŸ”½ Computational complexity

ğŸ”½ Memory access overhead

when compared to standard convolutional neural networks, making it ideal for embedded workloads.

â±ï¸ Inference Latency Considerations

The model is optimized for inference rather than training, enabling efficient forward passes suitable for near real-time defect screening at the edge.

Phase-1 focuses on:

âœ… Inference feasibility

âœ… Architectural suitability for low-latency execution

â³ Latency benchmarking and optimization are intentionally deferred to later phases.

ğŸ”„ Framework & Portability

To enable cross-platform deployment, the trained TensorFlow model is exported to the ONNX format.

ONNX allows inference using lightweight runtimes such as ONNX Runtime, enabling:

ğŸ” Framework-agnostic deployment

ğŸŒ Portability across heterogeneous edge platforms

without dependency on a specific deep learning framework.

ğŸ”Œ Offline Execution

The system supports fully offline inference, where all processing occurs locally on the device without cloud connectivity.

This is critical for industrial inspection systems requiring:

ğŸ•’ Deterministic latency

ğŸ›¡ï¸ High reliability

ğŸ“ Data locality

ğŸ› ï¸ Phase-2 Optimization Scope

Future work in Phase-2 includes edge-specific optimizations such as:

ğŸ”§ Model fine-tuning

ğŸ“‰ Post-training quantization

âœ‚ï¸ Pruning and operator-level optimization

These steps aim to further reduce inference latency and memory usage while preserving defect classification accuracy and recall.
